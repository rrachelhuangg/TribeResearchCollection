{
    "https://aclanthology.org/2025.findings-acl.1100.pdf": "An increasing adoption of Large Language Models (LLMs) in complex reasoning tasks necessitates their interpretability and reliability. Recent advances to this end include retrieval-augmented generation (RAG) and knowledge graph-enhanced RAG (GraphRAG), whereas they are constrained by static knowledge bases and ineffective multimodal data integration. In response, we propose a Query-Driven Multimodal GraphRAG framework that dynamically constructs local knowledge graphs tailored to query semantics. Our approach derives graph patterns from query semantics to guide knowledge extraction, employs a multi-path retrieval strategy to pinpoint core knowledge, and supplements missing multimodal information ad hoc. Experimental results on the MultimodalQA and WebQA datasets demonstrate that our framework achieves the state-of-the-art performance among unsupervised competitors, particularly excelling in cross-modal understanding of complex queries.",
    "https://proceedings.neurips.cc/paper_files/paper/2024/file/47eb2874a790d5b1f554b9bb93b3de9d-Paper-Conference.pdf": "Label Distribution Learning (LDL) has been extensively studied in IID data applications such as computer vision, thanks to its more generic setting over single-label and multi-label classification. This paper advances LDL into graph domains and aims to tackle a novel and fundamental heterogeneous graph label distribution learning (HGDL) problem. We argue that the graph heterogeneity reflected on node types, node attributes, and neighborhood structures can impose significant challenges for generalizing LDL onto graphs. To address the challenges, we propose a new learning framework with two key components: 1) proactive graph topology homogenization, and 2) topology and content consistency-aware graph transformer. Specifically, the former learns optimal information aggregation between meta-paths, so that the node heterogeneity can be proactively addressed prior to the succeeding embedding learning; the latter leverages an attention mechanism to learn consistency between meta-path and node attributes, allowing network topology and nodal attributes to be equally emphasized during the label distribution learning. By using KL-divergence and additional constraints, HGDL delivers an end-to-end solution for learning and predicting label distribution for nodes. Both theoretical and empirical studies substantiate the effectiveness of our HGDL approach. Our code and datasets are available at https://github.com/Listener-Watcher/HGDL.",
    "https://drops.dagstuhl.de/storage/00lipics/lipics-vol313-ecoop2024/LIPIcs.ECOOP.2024.21/LIPIcs.ECOOP.2024.21.pdf": "Gradual typing has emerged as a promising typing discipline for reconciling static and dynamic typing, which have respective strengths and shortcomings. Thanks to its promises, gradual typing has gained tremendous momentum in both industry and academia. A main challenge in gradual typing is that, however, the performance of its programs can often be unpredictable, and adding or removing the type of a a single parameter may lead to wild performance swings. Many approaches have been proposed to optimize gradual typing performance, but little work has been done to aid the understanding of the performance landscape of gradual typing and navigating the migration process (which adds type annotations to make programs more static) to avert performance slowdowns.",
    "https://www.ijcai.org/proceedings/2023/0745.pdf": "This paper reviews recent breakthroughs that strived to enable Online Learning (OL) in open feature spaces, referred to as Utilitarian Online Learning (UOL). It taxonomizes existing UOL models into three categories, analyzes their pros and cons, and discusses their application scenarios. The paper also benchmarks the performance of representative UOL models, highlighting open problems, challenges, and potential future directions of this emerging topic.",
    "https://jd92.wang/assets/files/vr24.pdf": "This paper exploits ubiquitous desktop interaction data as an input source for generating virtual reality (VR) interaction data, which can benefit tasks like user behavior analysis and experience enhancement. Time-varying stroke gestures are selected as the primary focus because of their prevalence across various applications and their diverse patterns.",
    "https://jd92.wang/assets/files/iccv23.pdf": "Deep neural networks are susceptible to adversarial examples, posing a significant security risk in critical applications. Adversarial Training (AT) is a well-established technique to enhance adversarial robustness, but it often comes at the cost of decreased generalization ability. This paper proposes Robustness Critical Fine-Tuning (RiFT), a novel approach to enhance generalization without compromising adversarial robustness.",
    "https://arxiv.org/pdf/2206.08516.pdf": "Federated learning has attracted increasing attention to building models without accessing the raw user data, especially in healthcare. In real applications, different federations can seldom work together due to possible reasons such as data heterogeneity and distrust/inexistence of the central server. In this paper, we propose a novel framework called MetaFed to facilitate trustworthy FL between different federations. MetaFed obtains a personalized model for each federation without a central server via the proposed Cyclic Knowledge Distillation. Specifically, MetaFed treats each federation as a meta distribution and aggregates knowledge of each federation in a cyclic manner. The training is split into two parts: common knowledge accumulation and personalization. Comprehensive experiments on seven benchmarks demonstrate that MetaFed without a server achieves better accuracy compared to state-of-the-art methods (e.g., $10 % +$ accuracy improvement compared to the baseline for PAMAP2) with fewer communication costs. More importantly, MetaFed shows remarkable performance in real healthcare-related applications.",
    "https://jd92.wang/assets/files/coling22.pdf": "Target-oriented Opinion Words Extraction (TOWE) is a fine-grained sentiment analysis task that aims to extract the corresponding opinion words of a given opinion target from the sentence. Recently, deep learning approaches have made remarkable progress on this task. Nevertheless, the TOWE task still suffers from the scarcity of training data due to the expensive data annotation process. Limited labeled data increase the risk of distribution shift between test data and training data. In this paper, we propose exploiting massive unlabeled data to reduce the risk by increasing the exposure of the model to varying distribution shifts. Specifically, we propose a novel Multi-Grained Consistency Regularization (MGCR) method to make use of unlabeled data and design two filters specifically for TOWE to filter noisy data at different granularity. Extensive experimental results on four TOWE benchmark datasets indicate the superiority of MGCR compared with current state-of-the-art methods.",
    "https://jd92.wang/assets/files/DG_survey_TKDE22.pdf": "Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets, applications, and our open-sourced codebase for fair evaluation. Finally, we summarize existing literature and present some potential research topics for the future.",
    "https://haipeng-chen.github.io/files/RL-tutorial-AIEIC.pdf": "Reinforcement Learning tutorial covering various aspects such as the RL problem, value-based and policy-based methods, Q-learning, DQN, and practical implementations.",
    "https://haipeng-chen.github.io/files/25cikm_AutoRuleSQL.pdf": "Natural Language to SQL (NL2SQL) enables natural language access to structured data, but LLM-based methods can be inefficient for real-time use and repetitive query patterns. We present AutoRuleSQL, a hybrid system that combines template-based fast paths with LLM fallback and offline bootstrapping. Empirical results show that it reduces latency by over $1 2 . 6 %$ and improves execution accuracy by up to $4 . 0 %$ , when combined with existing NL2SQL methods.",
    "https://haipeng-chen.github.io/files/25iclr-RL-SSCO.pdf": "Reinforcement learning (RL) has emerged as a promising tool for combinatorial optimization (CO) problems due to its ability to learn fast, effective, and generalizable solutions. Nonetheless, existing works mostly focus on one-shot deterministic CO, while sequential stochastic CO (SSCO) has rarely been studied despite its broad applications such as adaptive influence maximization (IM) and infectious disease intervention. In this paper, we study the SSCO problem where we first decide the budget (e.g., number of seed nodes in adaptive IM) allocation for all time steps, and then select a set of nodes for each time step. The few existing studies on SSCO simplify the problems by assuming a uniformly distributed budget allocation over the time horizon, yielding suboptimal solutions. We propose a generic hierarchical RL (HRL) framework called wake-sleep option (WS-option), a two-layer option-based framework that simultaneously decides adaptive budget allocation on the higher layer and node selection on the lower layer. WS-option starts with a coherent formulation of the two-layer Markov decision processes (MDPs), capturing the interdependencies between the two layers of decisions. Building on this, WS-option employs several innovative designs to balance the model\u2019s training stability and computational efficiency, preventing the vicious cyclic interference issue between the two layers. Empirical results show that WS-option exhibits significantly improved effectiveness and generalizability compared to traditional methods. Moreover, the learned model can be generalized to larger graphs, which significantly reduces the overhead of computational resources.",
    "https://haipeng-chen.github.io/files/25iclr-RL-ACESS.pdf": "This paper defines and studies a new class of asymmetric games called two-player Asymmetric Combinatorial-Continuous zEro-Sum (ACCES) games, featuring a combinatorial action space for one player and an infinite compact space for the other. The authors prove the existence of Nash equilibrium for these games, design a novel algorithm called Combinatorial Continuous DO (CCDO) to solve them, and propose a practical algorithm CCDORL based on reinforcement learning. Experimental results demonstrate the effectiveness of the proposed algorithms.",
    "https://haipeng-chen.github.io/files/25aaai-pad-ts.pdf": "Diffusion models have shown promising ability in generating high-quality time series (TS) data. Despite the initial success, existing works mostly focus on the authenticity of data at the individual level, but pay less attention to preserving the population-level properties on the entire dataset. Such population-level properties include value distributions for each dimension and distributions of certain functional dependencies (e.g., cross-correlation, CC) between different dimensions. For instance, when generating house energy consumption TS data, the value distributions of the outside temperature and the kitchen temperature should be preserved, as well as the distribution of CC between them. Preserving such TS population-level properties is critical in maintaining the statistical insights of the datasets, mitigating model bias, and augmenting downstream tasks like TS prediction. Yet, it is often overlooked by existing models. Hence, data generated by existing models often bear distribution shifts from the original data. We propose Population-aware Diffusion for Time Series (PaD-TS), a new TS generation model that better preserves the population-level properties. The key novelties of PaD-TS include 1) a new training method explicitly incorporating TS population-level property preservation, and 2) a new dual-channel encoder model architecture that better captures the TS data structure. Empirical results in major benchmark datasets show that PaD-TS can improve the average CC distribution shift score between real and synthetic data by 5.9x while maintaining a performance comparable to state-of-the-art models on individual-level authenticity.",
    "https://aclanthology.org/2025.findings-acl.331.pdf": "This study investigates the self-correction capabilities of Vision-Language Models (VLMs) during both inference and fine-tuning stages. It introduces a Self-Correction Learning (SCL) approach that enables VLMs to learn from their self-generated self-correction data through Direct Preference Optimization (DPO) without relying on external feedback, facilitating self-improvement. Experimental results demonstrate that while VLMs struggle to self-correct effectively during iterative inference without additional fine-tuning and external feedback, they can enhance their performance and avoid previous mistakes through preference fine-tuning.",
    "https://aclanthology.org/2025.findings-naacl.152.pdf": "Large Language Models (LLMs) are pretrained on extensive multilingual corpora to acquire both language-specific cultural knowledge and general knowledge. This paper explores the Cross-Lingual Self-Aligning ability of Language Models (CALM) to align knowledge across languages, enhancing cross-lingual knowledge question answering and demonstrating effectiveness in zero-shot and retrieval-augmented settings.",
    "https://aclanthology.org/2024.acl-long.18.pdf": "We explore and enhance the ability of neural language models to generate novel scientific directions grounded in literature. Work on literature-based hypothesis generation has traditionally focused on binary link prediction\u2014 severely limiting the expressivity of hypotheses. This line of work also does not focus on optimizing novelty. We take a dramatic departure with a novel setting in which models use as input background contexts (e.g., problems, experimental settings, goals), and output natural language ideas grounded in literature.",
    "https://eaglew.github.io/files/SciMON_ACL.pdf": "This document discusses the SciMON project, which aims to enhance human innovation through scientific literature discovery and knowledge acquisition using advanced machine learning techniques.",
    "https://eaglew.github.io/files/ACL2024-poster.pdf": "Qingyun Wang1, Doug Downey2, Heng Ji2, Tom Hope2,3 1University of Illinois at Urbana-Champaign, 2Allen Institute for Artificial Intelligence (AI2) 3The Hebrew University of Jerusalem"
}